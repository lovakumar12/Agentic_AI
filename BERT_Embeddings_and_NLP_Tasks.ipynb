{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4b37e6d",
   "metadata": {},
   "source": [
    "# BERT Embeddings and NLP Tasks\n",
    "\n",
    "In this notebook we will learn:\n",
    "\n",
    "- What BERT and BERT embeddings are  \n",
    "- Why contextual embeddings are better than static embeddings  \n",
    "- How to use BERT for:\n",
    "  - Text classification\n",
    "  - Named Entity Recognition (NER)\n",
    "  - Sentiment analysis\n",
    "\n",
    "We will use the Hugging Face `transformers` library and simple toy examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4787f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers torch datasets sentencepiece\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification\n",
    "from transformers import AutoModelForTokenClassification, pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e55f487",
   "metadata": {},
   "source": [
    "## 1. BERT and contextual embeddings\n",
    "\n",
    "BERT (Bidirectional Encoder Representations from Transformers) is a transformer-based language model that reads text in both directions to build contextual representations.\n",
    "\n",
    "An embedding is a dense numeric vector representing text meaning.\n",
    "\n",
    "BERT embeddings combine token, position, and segment embeddings and pass them through transformer layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02cebb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "bert_model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "sentence = \"BERT embeddings capture contextual meaning.\"\n",
    "\n",
    "inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    outputs = bert_model(**inputs)\n",
    "\n",
    "last_hidden_state = outputs.last_hidden_state\n",
    "cls_embedding = last_hidden_state[:, 0, :]\n",
    "\n",
    "print(\"Shape of last_hidden_state:\", last_hidden_state.shape)\n",
    "print(\"Shape of [CLS] embedding:\", cls_embedding.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9bca20f",
   "metadata": {},
   "source": [
    "## Contextual vs Static Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db2e1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent1 = \"The bank approved my loan.\"\n",
    "sent2 = \"We sat by the bank of the river.\"\n",
    "\n",
    "batch = tokenizer([sent1, sent2], padding=True, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    out = bert_model(**batch)\n",
    "\n",
    "for i, sent in enumerate([sent1, sent2]):\n",
    "    tokens = tokenizer.convert_ids_to_tokens(batch[\"input_ids\"][i])\n",
    "    print(sent)\n",
    "    print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c1a7b0",
   "metadata": {},
   "source": [
    "## Sentence Embeddings via Mean Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d563263",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output.last_hidden_state\n",
    "    mask = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return (token_embeddings * mask).sum(dim=1) / mask.sum(dim=1)\n",
    "\n",
    "sentences = [\"I love NLP.\", \"BERT embeddings are powerful.\"]\n",
    "enc = tokenizer(sentences, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    out = bert_model(**enc)\n",
    "\n",
    "embeddings = mean_pooling(out, enc[\"attention_mask\"])\n",
    "print(embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68ef573",
   "metadata": {},
   "source": [
    "## Text Classification with BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1b54fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "clf_tokenizer = AutoTokenizer.from_pretrained(clf_model_name)\n",
    "clf_model = AutoModelForSequenceClassification.from_pretrained(clf_model_name)\n",
    "\n",
    "text = \"This course is very interesting.\"\n",
    "inputs = clf_tokenizer(text, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    logits = clf_model(**inputs).logits\n",
    "\n",
    "probs = torch.softmax(logits, dim=-1)[0]\n",
    "print(clf_model.config.id2label, probs.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280e7c1e",
   "metadata": {},
   "source": [
    "## Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e16087",
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_pipe = pipeline(\"ner\", model=\"dslim/bert-base-NER\", aggregation_strategy=\"simple\")\n",
    "text = \"Barack Obama was born in Hawaii.\"\n",
    "print(ner_pipe(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a878820b",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26734639",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_pipe = pipeline(\"sentiment-analysis\")\n",
    "sentiment_pipe([\"I love this!\", \"This is terrible.\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19781809",
   "metadata": {},
   "source": [
    "## Recap\n",
    "\n",
    "You learned how to generate embeddings and apply BERT to classification, NER, and sentiment analysis."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}