{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af76573a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aac42011",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "openai_api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "groq_api_key = os.environ[\"GROQ_API_KEY\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afcc4927",
   "metadata": {},
   "source": [
    "## using open ai\n",
    "### completion model for completion like writing a blog ,poem anything it just end the by its generation of output \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c904234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The capital of India is New Delhi.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import OpenAI\n",
    "llm=OpenAI(temperature=0)\n",
    "response=llm.invoke(\"What is the capital of India?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab1289b",
   "metadata": {},
   "source": [
    "### chat model for conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "057d214d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "chatModel=ChatOpenAI(model=\"gpt-3.5-turbo-0125\",temperature=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "54e051fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    (\"system\", \"You are an historian expert in the India.\"),\n",
    "    (\"human\", \"Tell me one curious thing about India.\"),\n",
    "]\n",
    "response = chatModel.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d5ff8f20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"One curious thing about India is that it is home to the world's largest sundial, located in the city of Jaipur. Known as the Jantar Mantar, this astronomical observation site was built in the early 18th century and consists of a collection of architectural astronomical instruments. The sundial at Jantar Mantar is accurate to within 20 seconds, making it a remarkable feat of ancient Indian engineering and astronomy.\" additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 85, 'prompt_tokens': 28, 'total_tokens': 113, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-CNVf5OU4O7OCcGMJlksLBXvwPskpB', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--38b95c6e-b1f5-4ea2-b415-c319afd3f5e6-0' usage_metadata={'input_tokens': 28, 'output_tokens': 85, 'total_tokens': 113, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5695d02d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"One curious thing about India is that it is home to the world's largest sundial, located in the city of Jaipur. Known as the Jantar Mantar, this astronomical observation site was built in the early 18th century and consists of a collection of architectural astronomical instruments. The sundial at Jantar Mantar is accurate to within 20 seconds, making it a remarkable feat of ancient Indian engineering and astronomy.\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a2267c4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_usage': {'completion_tokens': 85,\n",
       "  'prompt_tokens': 28,\n",
       "  'total_tokens': 113,\n",
       "  'completion_tokens_details': {'accepted_prediction_tokens': 0,\n",
       "   'audio_tokens': 0,\n",
       "   'reasoning_tokens': 0,\n",
       "   'rejected_prediction_tokens': 0},\n",
       "  'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}},\n",
       " 'model_name': 'gpt-3.5-turbo-0125',\n",
       " 'system_fingerprint': None,\n",
       " 'id': 'chatcmpl-CNVf5OU4O7OCcGMJlksLBXvwPskpB',\n",
       " 'service_tier': 'default',\n",
       " 'finish_reason': 'stop',\n",
       " 'logprobs': None}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.response_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "599e18e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'$defs': {'InputTokenDetails': {'description': 'Breakdown of input token counts.\\n\\nDoes *not* need to sum to full input token count. Does *not* need to have all keys.\\n\\nExample:\\n    .. code-block:: python\\n\\n        {\\n            \"audio\": 10,\\n            \"cache_creation\": 200,\\n            \"cache_read\": 100,\\n        }\\n\\n.. versionadded:: 0.3.9\\n\\nMay also hold extra provider-specific keys.',\n",
       "   'properties': {'audio': {'title': 'Audio', 'type': 'integer'},\n",
       "    'cache_creation': {'title': 'Cache Creation', 'type': 'integer'},\n",
       "    'cache_read': {'title': 'Cache Read', 'type': 'integer'}},\n",
       "   'title': 'InputTokenDetails',\n",
       "   'type': 'object'},\n",
       "  'InvalidToolCall': {'description': 'Allowance for errors made by LLM.\\n\\nHere we add an ``error`` key to surface errors made during generation\\n(e.g., invalid JSON arguments.)',\n",
       "   'properties': {'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'title': 'Name'},\n",
       "    'args': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'title': 'Args'},\n",
       "    'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'title': 'Id'},\n",
       "    'error': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'title': 'Error'},\n",
       "    'type': {'const': 'invalid_tool_call', 'title': 'Type', 'type': 'string'}},\n",
       "   'required': ['name', 'args', 'id', 'error'],\n",
       "   'title': 'InvalidToolCall',\n",
       "   'type': 'object'},\n",
       "  'OutputTokenDetails': {'description': 'Breakdown of output token counts.\\n\\nDoes *not* need to sum to full output token count. Does *not* need to have all keys.\\n\\nExample:\\n    .. code-block:: python\\n\\n        {\\n            \"audio\": 10,\\n            \"reasoning\": 200,\\n        }\\n\\n.. versionadded:: 0.3.9',\n",
       "   'properties': {'audio': {'title': 'Audio', 'type': 'integer'},\n",
       "    'reasoning': {'title': 'Reasoning', 'type': 'integer'}},\n",
       "   'title': 'OutputTokenDetails',\n",
       "   'type': 'object'},\n",
       "  'ToolCall': {'description': 'Represents a request to call a tool.\\n\\nExample:\\n\\n    .. code-block:: python\\n\\n        {\"name\": \"foo\", \"args\": {\"a\": 1}, \"id\": \"123\"}\\n\\n    This represents a request to call the tool named ``\\'foo\\'`` with arguments\\n    ``{\"a\": 1}`` and an identifier of ``\\'123\\'``.',\n",
       "   'properties': {'name': {'title': 'Name', 'type': 'string'},\n",
       "    'args': {'additionalProperties': True, 'title': 'Args', 'type': 'object'},\n",
       "    'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'title': 'Id'},\n",
       "    'type': {'const': 'tool_call', 'title': 'Type', 'type': 'string'}},\n",
       "   'required': ['name', 'args', 'id'],\n",
       "   'title': 'ToolCall',\n",
       "   'type': 'object'},\n",
       "  'UsageMetadata': {'description': 'Usage metadata for a message, such as token counts.\\n\\nThis is a standard representation of token usage that is consistent across models.\\n\\nExample:\\n    .. code-block:: python\\n\\n        {\\n            \"input_tokens\": 350,\\n            \"output_tokens\": 240,\\n            \"total_tokens\": 590,\\n            \"input_token_details\": {\\n                \"audio\": 10,\\n                \"cache_creation\": 200,\\n                \"cache_read\": 100,\\n            },\\n            \"output_token_details\": {\\n                \"audio\": 10,\\n                \"reasoning\": 200,\\n            },\\n        }\\n\\n.. versionchanged:: 0.3.9\\n\\n    Added ``input_token_details`` and ``output_token_details``.',\n",
       "   'properties': {'input_tokens': {'title': 'Input Tokens', 'type': 'integer'},\n",
       "    'output_tokens': {'title': 'Output Tokens', 'type': 'integer'},\n",
       "    'total_tokens': {'title': 'Total Tokens', 'type': 'integer'},\n",
       "    'input_token_details': {'$ref': '#/$defs/InputTokenDetails'},\n",
       "    'output_token_details': {'$ref': '#/$defs/OutputTokenDetails'}},\n",
       "   'required': ['input_tokens', 'output_tokens', 'total_tokens'],\n",
       "   'title': 'UsageMetadata',\n",
       "   'type': 'object'}},\n",
       " 'additionalProperties': True,\n",
       " 'description': 'Message from an AI.\\n\\nAIMessage is returned from a chat model as a response to a prompt.\\n\\nThis message represents the output of the model and consists of both\\nthe raw output as returned by the model together standardized fields\\n(e.g., tool calls, usage metadata) added by the LangChain framework.',\n",
       " 'properties': {'content': {'anyOf': [{'type': 'string'},\n",
       "    {'items': {'anyOf': [{'type': 'string'},\n",
       "       {'additionalProperties': True, 'type': 'object'}]},\n",
       "     'type': 'array'}],\n",
       "   'title': 'Content'},\n",
       "  'additional_kwargs': {'additionalProperties': True,\n",
       "   'title': 'Additional Kwargs',\n",
       "   'type': 'object'},\n",
       "  'response_metadata': {'additionalProperties': True,\n",
       "   'title': 'Response Metadata',\n",
       "   'type': 'object'},\n",
       "  'type': {'const': 'ai', 'default': 'ai', 'title': 'Type', 'type': 'string'},\n",
       "  'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "   'default': None,\n",
       "   'title': 'Name'},\n",
       "  'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "   'default': None,\n",
       "   'title': 'Id'},\n",
       "  'example': {'default': False, 'title': 'Example', 'type': 'boolean'},\n",
       "  'tool_calls': {'default': [],\n",
       "   'items': {'$ref': '#/$defs/ToolCall'},\n",
       "   'title': 'Tool Calls',\n",
       "   'type': 'array'},\n",
       "  'invalid_tool_calls': {'default': [],\n",
       "   'items': {'$ref': '#/$defs/InvalidToolCall'},\n",
       "   'title': 'Invalid Tool Calls',\n",
       "   'type': 'array'},\n",
       "  'usage_metadata': {'anyOf': [{'$ref': '#/$defs/UsageMetadata'},\n",
       "    {'type': 'null'}],\n",
       "   'default': None}},\n",
       " 'required': ['content'],\n",
       " 'title': 'AIMessage',\n",
       " 'type': 'object'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.model_json_schema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1120318",
   "metadata": {},
   "source": [
    "## using groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f2f138",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07093963",
   "metadata": {},
   "outputs": [],
   "source": [
    "## using llama3 70B model from Groq\n",
    "from langchain_groq import ChatGroq\n",
    "llama_llm = ChatGroq(model=\"llama3-70b-8192\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e666f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Mistral 8x7B\n",
    "mistralChatModel = ChatGroq(\n",
    "    model=\"mixtral-8x7b-32768\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721a769e",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    (\"system\", \"You are an historian expert in the india.\"),\n",
    "    (\"human\", \"How many members  are there in parlament?\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5df69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = mistralChatModel.invoke(messages)\n",
    "print(response) \n",
    "response= llama_llm.invoke(\"What is the capital of India?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c243a5",
   "metadata": {},
   "source": [
    "##### Ref::https://python.langchain.com/docs/integrations/chat/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5e22fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d9ca10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
